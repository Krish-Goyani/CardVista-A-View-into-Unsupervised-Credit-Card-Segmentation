{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationandClusteringConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    metrics_file_name: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCardVista\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCardVista\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_yaml, create_directories\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfigurationManager\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.CardVista.constants import *\n",
    "from src.CardVista.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_and_clustering_config(self)->DataTransformationandClusteringConfig:\n",
    "        config = self.config.data_transformation_and_clustering\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_and__clustering_config=DataTransformationandClusteringConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            metrics_file_name= config.metrics_file_name\n",
    "\n",
    "        )\n",
    "\n",
    "        return data_transformation_and__clustering_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CardVista.logging import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "from src.CardVista.utils.common import read_yaml, create_directories, save_json\n",
    "\n",
    "class ReplaceOutliers(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "    \n",
    "    \n",
    "\n",
    "    def transform(self, X):\n",
    "        schema_filepath = SCHEMA_FILE_PATH\n",
    "        schema = read_yaml(schema_filepath)\n",
    "        df = pd.DataFrame(X, columns=schema.COLUMNS)\n",
    "        \n",
    "        for col in list(df.columns):\n",
    "            q1 = df[col].quantile(0.15)\n",
    "            q3 = df[col].quantile(0.85)\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            upper_limit = q3 + 1.5 * iqr\n",
    "            lower_limit = q1 - 1.5 * iqr\n",
    "            \n",
    "            df.loc[df[col] > upper_limit, col] = upper_limit\n",
    "            df.loc[df[col] < lower_limit, col] = lower_limit\n",
    "            \n",
    "        return df.to_numpy()\n",
    "\n",
    "class DataTransformationandClustering:\n",
    "    def __init__(self, config: DataTransformationandClusteringConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def eval_metrics(self,data,labels):\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "        davies_bouldin = davies_bouldin_score(data, labels)\n",
    "\n",
    "        return silhouette,calinski_harabasz ,davies_bouldin\n",
    "\n",
    "\n",
    "\n",
    "    def data_transformation_and_clustering(self):\n",
    "        logger.info(\"data transformation started\")\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        data.drop('CUST_ID',axis=1,inplace=True)\n",
    "        temp = data.copy()\n",
    "        replace_outliers = ReplaceOutliers()\n",
    "\n",
    "        trf1 = SimpleImputer(strategy='median')\n",
    "        trf2 = replace_outliers\n",
    "        trf3 = StandardScaler()\n",
    "        trf4 = KMeans(n_clusters=3,random_state = 42,n_init = \"auto\")\n",
    "\n",
    "        pipe = Pipeline([('trf1',trf1),\n",
    "                 ('trf2',trf2),\n",
    "                 ('trf3',trf3),\n",
    "                 ('trf4',trf4)\n",
    "                ])\n",
    "        logger.info(\"data fitted in pipeline\")\n",
    "        pipe.fit(data)\n",
    "\n",
    "\n",
    "        partial_pipe = Pipeline([('trf1', trf1), \n",
    "                         ('trf2',trf2),\n",
    "                         ('trf3', trf3)])\n",
    "        \n",
    "        logger.info(\"data transformed in partial pipeline\")\n",
    "        scaled_data = partial_pipe.transform(temp)\n",
    "\n",
    "\n",
    "        labels = pipe.fit_predict(data)\n",
    "\n",
    "\n",
    "        creditcard_df_scaled=scaled_data\n",
    "        labels = labels\n",
    "\n",
    "        (silhouette,calinski_harabasz,davies_bouldin) = self.eval_metrics(scaled_data,labels)\n",
    "        scores = {\"silhouette_score\" : silhouette,\"calinski_harabasz_score\" : calinski_harabasz,\"davies_bouldin_score\" : davies_bouldin}\n",
    "        save_json(path=Path(self.config.metrics_file_name),data=scores)\n",
    "        logger.info(\"KMeans evalution metrics saved\")\n",
    "    \n",
    "        data['CLUSTER'] = labels\n",
    "\n",
    "        # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "        train, test = train_test_split(data)\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
    "\n",
    "        logger.info(\"Splited data into training and test sets\")\n",
    "        logger.info(train.shape)\n",
    "        logger.info(test.shape)\n",
    "\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-16 19:34:40,953 : INFO : common : yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-16 19:34:40,953 : INFO : common : yaml file: params.yaml loaded successfully]\n",
      "[2024-02-16 19:34:40,958 : INFO : common : yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-16 19:34:40,961 : INFO : common : created directory at: artifacts]\n",
      "[2024-02-16 19:34:40,961 : INFO : common : created directory at: artifacts/data_transformation]\n",
      "[2024-02-16 19:34:40,962 : INFO : 26221436 : data transformation started]\n",
      "[2024-02-16 19:34:40,994 : INFO : 26221436 : data fitted in pipeline]\n",
      "[2024-02-16 19:34:41,018 : INFO : common : yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-16 19:34:41,092 : INFO : 26221436 : data transformed in partial pipeline]\n",
      "[2024-02-16 19:34:41,094 : INFO : common : yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-16 19:34:41,158 : INFO : common : yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-16 19:34:44,006 : INFO : common : json file saved at: artifacts\\data_transformation\\metrics.json]\n",
      "[2024-02-16 19:34:44,006 : INFO : 26221436 : KMeans evalution metrics saved]\n",
      "[2024-02-16 19:34:44,143 : INFO : 26221436 : Splited data into training and test sets]\n",
      "[2024-02-16 19:34:44,143 : INFO : 26221436 : (6712, 18)]\n",
      "[2024-02-16 19:34:44,143 : INFO : 26221436 : (2238, 18)]\n",
      "(6712, 18)\n",
      "(2238, 18)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_and_clustering_config = config.get_data_transformation_and_clustering_config()\n",
    "    data_transformation_and_clustering_config = DataTransformationandClusteringConfig(config=data_transformation_and_clustering_config)\n",
    "    data_transformation_and_clustering_config.data_transformation_and_clustering()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
